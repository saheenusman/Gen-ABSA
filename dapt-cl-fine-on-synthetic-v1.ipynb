{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13665516,"sourceType":"datasetVersion","datasetId":8688595},{"sourceId":274723691,"sourceType":"kernelVersion"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#%%\n# =======================================================================\n# CELL 0: ONE-SHOT SETUP WITH KERNEL RESTART (REQUIRED FOR PROTOBUF)\n# =======================================================================\nprint(\"=\"*70)\nprint(\"COMPLETE SETUP - PROTOBUF FIX + KERNEL RESTART\")\nprint(\"=\"*70)\n\nimport subprocess\nimport sys\nimport os\n\n# Step 1: Uninstall protobuf completely\nprint(\"[1/4] Removing old protobuf...\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"protobuf\"], \n               capture_output=True, check=False)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"protobuf\"], \n               capture_output=True, check=False)  # Run twice to ensure clean\nprint(\"      âœ“ Old protobuf removed\")\n\n# Step 2: Install correct version\nprint(\"[2/4] Installing protobuf 3.20.3...\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"protobuf==3.20.3\"], \n               capture_output=True, check=True)\nprint(\"      âœ“ Protobuf 3.20.3 installed\")\n\n# Step 3: Install other packages\nprint(\"[3/4] Installing packages...\")\nsubprocess.run([\n    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n    \"pandas\", \"numpy\", \"scikit-learn\",\n    \"transformers\", \"torch\", \"accelerate\", \"peft\",\n    \"hdbscan\", \"umap-learn\", \"datasets\", \"sentencepiece\", \"evaluate\" , \"rouge_score\"  , \"datasets\"\n], capture_output=True, check=True)\nprint(\"      âœ“ Packages installed\")\n\n# Step 4: Restart kernel\nprint(\"[4/4] Restarting kernel...\")\nprint(\"\\n\" + \"=\"*70)\nprint(\"âš ï¸  KERNEL WILL RESTART NOW\")\nprint(\"âš ï¸  AFTER RESTART, RUN CELL 1 (NOT THIS CELL AGAIN)\")\nprint(\"=\"*70)\n\n# Restart IPython kernel\n# os._exit(0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:11:47.677519Z","iopub.execute_input":"2025-11-09T17:11:47.677750Z","iopub.status.idle":"2025-11-09T17:13:16.508222Z","shell.execute_reply.started":"2025-11-09T17:11:47.677728Z","shell.execute_reply":"2025-11-09T17:13:16.507436Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nCOMPLETE SETUP - PROTOBUF FIX + KERNEL RESTART\n======================================================================\n[1/4] Removing old protobuf...\n      âœ“ Old protobuf removed\n[2/4] Installing protobuf 3.20.3...\n      âœ“ Protobuf 3.20.3 installed\n[3/4] Installing packages...\n      âœ“ Packages installed\n[4/4] Restarting kernel...\n\n======================================================================\nâš ï¸  KERNEL WILL RESTART NOW\nâš ï¸  AFTER RESTART, RUN CELL 1 (NOT THIS CELL AGAIN)\n======================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#%%\n# =======================================================================\n# CELL 1: IMPORTS (RUN AFTER KERNEL RESTART)\n# =======================================================================\nprint(\"=\"*70)\nprint(\"CELL 1: IMPORTING LIBRARIES\")\nprint(\"=\"*70)\n\nimport json\nimport pandas as pd\nimport numpy as np\nimport os\nfrom collections import defaultdict, Counter\nimport torch\nfrom transformers import T5EncoderModel, AutoTokenizer, T5TokenizerFast, T5ForConditionalGeneration\nfrom peft import PeftModel\nfrom hdbscan import HDBSCAN\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nfrom datasets import Dataset\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"âœ… All libraries imported successfully\")\n\n# GPU check\nif torch.cuda.is_available():\n    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n    device = torch.device(\"cuda\")\nelse:\n    print(\"âš ï¸ No GPU - using CPU\")\n    device = torch.device(\"cpu\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… CELL 1 COMPLETE - Proceed to Cell 2\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:13:31.917111Z","iopub.execute_input":"2025-11-09T17:13:31.917713Z","iopub.status.idle":"2025-11-09T17:14:06.555675Z","shell.execute_reply.started":"2025-11-09T17:13:31.917689Z","shell.execute_reply":"2025-11-09T17:14:06.554746Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nCELL 1: IMPORTING LIBRARIES\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"2025-11-09 17:13:45.829923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762708426.022228      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762708426.074201      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"âœ… All libraries imported successfully\nâœ… GPU: Tesla T4\n\n======================================================================\nâœ… CELL 1 COMPLETE - Proceed to Cell 2\n======================================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from pathlib import Path\nimport torch\nimport os\n\n# Set the TSV folder\nTSV_DATA_DIR = Path(\"/kaggle/input/syntethic-absa\")\nassert TSV_DATA_DIR.exists(), f\"TSV_DATA_DIR missing: {TSV_DATA_DIR}\"\n\n# CL DAPT adapter\nDAPT_ADAPTER_DIR = Path(\"/kaggle/input/cl-training/t5-dapt-contrastive-finetuned\")\nassert DAPT_ADAPTER_DIR.exists(), f\"DAPT_ADAPTER_DIR missing: {DAPT_ADAPTER_DIR}\"\n\nOUTPUT_MODEL_DIR = Path(\"/kaggle/working/flan-t5-dapt-cl-synthetic-full-fft-optimized\")\nOUTPUT_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n\nBASE_MODEL_NAME = \"google/flan-t5-base\"\n\n# ============================================================================\n# HYPERPARAMETERS (OPTIMIZED - Based on Your Best Config)\n# ============================================================================\n# Sequence lengths (adjusted for 7-field format)\nMAX_INPUT_LENGTH = 384  # Shorter than 512 but safe for your prompts\nMAX_TARGET_LENGTH = 96  # Longer than 64 to fit 7 fields\n\n# Batch sizes (match your proven config)\nPER_DEVICE_TRAIN_BATCH_SIZE = 8  # Your best: 8\nGRADIENT_ACCUMULATION_STEPS = 4  # Your best: 4\nPER_DEVICE_EVAL_BATCH_SIZE = 8   # Your best: 8\n# Effective batch size = 8 * 4 = 32\n\n# Learning rate and optimizer (match your best)\nLEARNING_RATE = 2.5e-4  # Your best: 2.5e-4 (higher than current 1e-4)\nOPTIMIZER = \"adamw_torch\"\nWARMUP_RATIO = 0.0  # No warmup (like your best config)\nWEIGHT_DECAY = 0.0  # NO WEIGHT DECAY (your best had none)\n\n# Training duration\nNUM_TRAIN_EPOCHS = 5  # Same as before\n\n# Generation settings\nGEN_NUM_BEAMS = 5  # Keep at 5 (good balance)\n\n# Logging\nLOGGING_STEPS = 50\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nprint(f\"TSV data location: {TSV_DATA_DIR}\")\nprint(f\"CL DAPT adapter location: {DAPT_ADAPTER_DIR}\")\nprint(f\"Output directory: {OUTPUT_MODEL_DIR}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"OPTIMIZED HYPERPARAMETERS\")\nprint(\"=\"*70)\nprint(f\"Learning Rate:      {LEARNING_RATE} (2.5x higher than before)\")\nprint(f\"Weight Decay:       {WEIGHT_DECAY} (DISABLED - matches your best)\")\nprint(f\"Batch Size:         {PER_DEVICE_TRAIN_BATCH_SIZE} (2x larger than before)\")\nprint(f\"Effective Batch:    {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Max Input Length:   {MAX_INPUT_LENGTH} (25% shorter)\")\nprint(f\"Max Target Length:  {MAX_TARGET_LENGTH} (25% shorter)\")\nprint(f\"FP16:               Will be set to FALSE in training args\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:22:08.491147Z","iopub.execute_input":"2025-11-09T17:22:08.492373Z","iopub.status.idle":"2025-11-09T17:22:08.507635Z","shell.execute_reply.started":"2025-11-09T17:22:08.492344Z","shell.execute_reply":"2025-11-09T17:22:08.506927Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTSV data location: /kaggle/input/syntethic-absa\nCL DAPT adapter location: /kaggle/input/cl-training/t5-dapt-contrastive-finetuned\nOutput directory: /kaggle/working/flan-t5-dapt-cl-synthetic-full-fft-optimized\n\n======================================================================\nOPTIMIZED HYPERPARAMETERS\n======================================================================\nLearning Rate:      0.00025 (2.5x higher than before)\nWeight Decay:       0.0 (DISABLED - matches your best)\nBatch Size:         8 (2x larger than before)\nEffective Batch:    32\nMax Input Length:   384 (25% shorter)\nMax Target Length:  96 (25% shorter)\nFP16:               Will be set to FALSE in training args\n======================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# LOAD AND VERIFY TSV DATA (CORRECTED)\n# ============================================================================\nimport csv\nimport pandas as pd\nfrom datasets import Dataset\nfrom pathlib import Path\nimport re\n\ndef load_tsv_simple(path):\n    \"\"\"Load TSV with input/target columns, skip header\"\"\"\n    rows = []\n    print(f\"Reading from: {path.name}...\")\n    \n    with open(path, \"r\", encoding=\"utf-8\") as fh:\n        reader = csv.reader(fh, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n        header = next(reader, None)  # Skip header\n        \n        for row in reader:\n            if len(row) >= 2:\n                rows.append({\"input\": row[0], \"target\": row[1]})\n    \n    print(f\"Successfully loaded {len(rows)} rows from {path.name}.\")\n    return Dataset.from_pandas(pd.DataFrame(rows))\n\ndef extract_text_from_prompt(prompt_input):\n    \"\"\"\n    Extract the actual text being analyzed from the prompt.\n    Looks for the LAST occurrence of 'Text:' before 'Output:'\n    \"\"\"\n    # Strategy: Find all \"Text: <content>\" patterns, take the last one\n    all_matches = list(re.finditer(r'Text:\\s*(.+?)(?=\\s*Output:|$)', prompt_input, re.DOTALL))\n    \n    if all_matches:\n        # Return the LAST match (the actual text to analyze, not examples)\n        return all_matches[-1].group(1).strip()\n    \n    return \"[Could not extract text]\"\n\n# ============================================================================\n# LOAD DATASETS\n# ============================================================================\nTSV_DIR = Path(\"/kaggle/input/syntethic-absa\")\n\nprint(\"=\"*60)\nprint(\"Loading TSV datasets using Python's csv module...\")\nprint(\"=\"*60)\n\ntrain_ds = load_tsv_simple(TSV_DIR / \"train.tsv\")\nval_ds = load_tsv_simple(TSV_DIR / \"val.tsv\")\ntest_ds = load_tsv_simple(TSV_DIR / \"test.tsv\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Dataset sizes:\")\nprint(f\"  Train: {len(train_ds):,}\")\nprint(f\"  Val:   {len(val_ds):,}\")\nprint(f\"  Test:  {len(test_ds):,}\")\nprint(\"=\"*60)\n\n# ============================================================================\n# DISPLAY SAMPLE\n# ============================================================================\nprint(\"\\n--- Train Sample 0 ---\")\nsample = train_ds[0]\n\nprint(f\"INPUT:\")\nprint(sample['input'])\nprint(f\"\\n\\nTARGET:\")\nprint(repr(sample['target']))\n\n# Extract and show actual text\nactual_text = extract_text_from_prompt(sample['input'])\nprint(f\"\\n\\nEXTRACTED ACTUAL TEXT:\")\nprint(actual_text)\nprint(\"=\"*60)\n\n# ============================================================================\n# VERIFY MORE SAMPLES\n# ============================================================================\nprint(\"\\n--- Verification: First 5 Samples ---\")\nfor i in range(min(5, len(train_ds))):\n    sample = train_ds[i]\n    actual_text = extract_text_from_prompt(sample['input'])\n    target = sample['target']\n    \n    print(f\"\\n{'â”€'*60}\")\n    print(f\"Sample {i+1}:\")\n    print(f\"Text: {actual_text[:150]}...\")\n    print(f\"Target: {target[:150]}...\")\n    \n    # Count aspects\n    aspects = target.split(' ; ')\n    print(f\"Number of aspects: {len(aspects)}\")\n    \n    # Show first aspect term\n    first_aspect = aspects[0].split(' | ')[0] if aspects else \"\"\n    print(f\"First aspect: {first_aspect}\")\n\n# ============================================================================\n# SANITY CHECK: Verify text matches target\n# ============================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"SANITY CHECK: Do extracted texts match targets?\")\nprint(\"=\"*60)\n\nmismatch_count = 0\nfor i in range(min(10, len(train_ds))):\n    sample = train_ds[i]\n    actual_text = extract_text_from_prompt(sample['input']).lower()\n    target = sample['target'].lower()\n    \n    # Extract aspect from target\n    first_aspect = target.split(' | ')[0].split(';')[0].strip()\n    \n    # Check if aspect term or keywords appear in text\n    # For battery_drain, check for \"battery\" or \"drain\"\n    aspect_keywords = first_aspect.replace('_', ' ').split()\n    \n    found = any(keyword in actual_text for keyword in aspect_keywords if len(keyword) > 3)\n    \n    if not found:\n        mismatch_count += 1\n        print(f\"âš ï¸ Sample {i+1}: Aspect '{first_aspect}' not found in text\")\n        print(f\"   Text: {actual_text[:100]}...\")\n\nif mismatch_count == 0:\n    print(\"âœ… All samples verified - text and targets match!\")\nelse:\n    print(f\"âš ï¸ Found {mismatch_count} potential mismatches\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"âœ“ TSV Loading and Verification Complete\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:22:32.325854Z","iopub.execute_input":"2025-11-09T17:22:32.326389Z","iopub.status.idle":"2025-11-09T17:22:32.416205Z","shell.execute_reply.started":"2025-11-09T17:22:32.326363Z","shell.execute_reply":"2025-11-09T17:22:32.415484Z"}},"outputs":[{"name":"stdout","text":"============================================================\nLoading TSV datasets using Python's csv module...\n============================================================\nReading from: train.tsv...\nSuccessfully loaded 567 rows from train.tsv.\nReading from: val.tsv...\nSuccessfully loaded 122 rows from val.tsv.\nReading from: test.tsv...\nSuccessfully loaded 122 rows from test.tsv.\n\n============================================================\nDataset sizes:\n  Train: 567\n  Val:   122\n  Test:  122\n============================================================\n\n--- Train Sample 0 ---\nINPUT:\nExtract aspects and sentiments. Output format: aspect | sentiment | span | rationale | is_implicit | has_slang | has_emoji\nSeparate multiple aspects with ' ; '.\n\nExample 1:\nText: Great food, slow service.\nOutput:\nfood | positive | [1, 1] | User praises food. | FALSE | FALSE | FALSE ; service | negative | [3, 3] | User complains about service speed. | FALSE | FALSE | FALSE\n\nExample 2:\nText: ugh battery draining so fast ğŸ˜©\nOutput:\nbattery | negative | [2, 2] | Complaint about battery life. | FALSE | TRUE | TRUE\n\nExample 3:\nText: This place is beautiful and perfect for dates.\nOutput:\nambiance | positive | [] | Implicit praise of atmosphere. | TRUE | FALSE | FALSE\n\nExample 4:\nText: love the camera but battery life sucks ğŸ˜¤\nOutput:\ncamera | positive | [2, 2] | User likes camera. | FALSE | FALSE | TRUE ; battery_life | negative | [5, 6] | User dislikes battery using slang. | FALSE | TRUE | TRUE\n\nText: ugh my phone's battery is draining way too quick today, can't even make it thru lunch without charging ğŸ˜©\nOutput:\n\n\n\nTARGET:\n\"battery_drain | negative | [3, 3] | Explicit negative: 'battery is draining way too quick' directly complains about rapid power loss. | FALSE | FALSE | TRUE\"\n\n\nEXTRACTED ACTUAL TEXT:\nugh my phone's battery is draining way too quick today, can't even make it thru lunch without charging ğŸ˜©\n============================================================\n\n--- Verification: First 5 Samples ---\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSample 1:\nText: ugh my phone's battery is draining way too quick today, can't even make it thru lunch without charging ğŸ˜©...\nTarget: battery_drain | negative | [3, 3] | Explicit negative: 'battery is draining way too quick' directly complains about rapid power loss. | FALSE | FALSE ...\nNumber of aspects: 1\nFirst aspect: battery_drain\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSample 2:\nText: finally got that new laptop and the screen quality is actually crisp af, no more squinting at pixels lol...\nTarget: screen_quality | positive | [7, 7] | Explicit positive: 'screen quality is actually crisp' praises clear display. | FALSE | TRUE | FALSE...\nNumber of aspects: 1\nFirst aspect: screen_quality\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSample 3:\nText: customer service at this bank is a joke, waited 45 mins on hold just to get transferred smh...\nTarget: customer_service | negative | [0, 2] | Explicit negative: 'customer service is a joke' with wait time complaint shows frustration. | FALSE | FALSE | F...\nNumber of aspects: 1\nFirst aspect: customer_service\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSample 4:\nText: that movie's plot had me hooked from start, twists everywhere but didn't feel forced ngl ğŸ”¥...\nTarget: plot | positive | [1, 1] | Explicit positive: 'plot had me hooked' highlights engaging storyline. | FALSE | TRUE | TRUE...\nNumber of aspects: 1\nFirst aspect: plot\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSample 5:\nText: tax policy changes got me paying less this year, finally some relief in my pocket ğŸ’°...\nTarget: tax_policy | positive | [0, 2] | Explicit positive: 'tax policy changes got me paying less' expresses financial benefit. | FALSE | FALSE | TRUE...\nNumber of aspects: 1\nFirst aspect: tax_policy\n\n============================================================\nSANITY CHECK: Do extracted texts match targets?\n============================================================\nâœ… All samples verified - text and targets match!\n\n============================================================\nâœ“ TSV Loading and Verification Complete\n============================================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: LOAD TOKENIZER AND CL DAPT MODEL (FULL MODEL VERSION)\n# ============================================================================\nprint(\"=\"*70)\nprint(\"LOADING TOKENIZER AND CL DAPT MODEL\")\nprint(\"=\"*70)\n\nfrom transformers import T5TokenizerFast, T5ForConditionalGeneration\n\n# Load tokenizer\nprint(\"\\n[1/3] Loading tokenizer...\")\ntokenizer = T5TokenizerFast.from_pretrained(DAPT_ADAPTER_DIR)\nprint(\"âœ“ Tokenizer loaded\")\n\n# Load CL DAPT model directly (not as adapter)\nprint(\"\\n[2/3] Loading CL DAPT model (full model)...\")\nmodel = T5ForConditionalGeneration.from_pretrained(DAPT_ADAPTER_DIR)\nprint(\"âœ“ CL DAPT model loaded\")\n\n# Unfreeze all parameters for full fine-tuning\nprint(\"\\n[3/3] Preparing for fine-tuning...\")\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Move to GPU\nmodel = model.to(device)\n\n# Verify trainable params\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL READY\")\nprint(\"=\"*70)\nprint(f\"Total params:     {total_params:,}\")\nprint(f\"Trainable params: {trainable_params:,}\")\nprint(f\"Trainable %:      {100 * trainable_params / total_params:.2f}%\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:22:44.458799Z","iopub.execute_input":"2025-11-09T17:22:44.459577Z","iopub.status.idle":"2025-11-09T17:22:53.550304Z","shell.execute_reply.started":"2025-11-09T17:22:44.459549Z","shell.execute_reply":"2025-11-09T17:22:53.549439Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nLOADING TOKENIZER AND CL DAPT MODEL\n======================================================================\n\n[1/3] Loading tokenizer...\nâœ“ Tokenizer loaded\n\n[2/3] Loading CL DAPT model (full model)...\nâœ“ CL DAPT model loaded\n\n[3/3] Preparing for fine-tuning...\n\n======================================================================\nMODEL READY\n======================================================================\nTotal params:     247,577,856\nTrainable params: 247,577,856\nTrainable %:      100.00%\n======================================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================================\n# CELL 6: TOKENIZATION\n# ============================================================================\nprint(\"=\"*70)\nprint(\"TOKENIZING DATASETS\")\nprint(\"=\"*70)\n\ndef preprocess_function(batch):\n    \"\"\"\n    Tokenize input/target pairs.\n    Input: Full prompt with examples + actual text\n    Target: aspect | sentiment | span | rationale | is_implicit | has_slang | has_emoji\n    \"\"\"\n    # Tokenize inputs (prompts)\n    inputs = tokenizer(\n        batch[\"input\"], \n        max_length=MAX_INPUT_LENGTH,\n        truncation=True,\n        padding=False\n    )\n    \n    # Tokenize targets (7-field format)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            batch[\"target\"], \n            max_length=MAX_TARGET_LENGTH,\n            truncation=True,\n            padding=False\n        )\n    \n    inputs[\"labels\"] = labels[\"input_ids\"]\n    return inputs\n\nprint(\"\\nTokenizing train set...\")\ntokenized_train = train_ds.map(\n    preprocess_function, \n    batched=True, \n    remove_columns=train_ds.column_names,\n    desc=\"Tokenizing train\"\n)\n\nprint(\"Tokenizing validation set...\")\ntokenized_val = val_ds.map(\n    preprocess_function, \n    batched=True, \n    remove_columns=val_ds.column_names,\n    desc=\"Tokenizing val\"\n)\n\nprint(\"Tokenizing test set...\")\ntokenized_test = test_ds.map(\n    preprocess_function, \n    batched=True, \n    remove_columns=test_ds.column_names,\n    desc=\"Tokenizing test\"\n)\n\n# Check token lengths\ntrain_input_lens = [len(x) for x in tokenized_train['input_ids']]\ntrain_label_lens = [len(x) for x in tokenized_train['labels']]\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TOKENIZATION STATISTICS\")\nprint(\"=\"*70)\nprint(f\"Input tokens - Min: {min(train_input_lens)}, Max: {max(train_input_lens)}, Avg: {sum(train_input_lens)/len(train_input_lens):.1f}\")\nprint(f\"Target tokens - Min: {min(train_label_lens)}, Max: {max(train_label_lens)}, Avg: {sum(train_label_lens)/len(train_label_lens):.1f}\")\n\ntruncated_inputs = sum(1 for l in train_input_lens if l == MAX_INPUT_LENGTH)\ntruncated_targets = sum(1 for l in train_label_lens if l == MAX_TARGET_LENGTH)\n\nprint(f\"\\nTruncation: Inputs {truncated_inputs}/{len(train_input_lens)} ({truncated_inputs/len(train_input_lens)*100:.1f}%), Targets {truncated_targets}/{len(train_label_lens)} ({truncated_targets/len(train_label_lens)*100:.1f}%)\")\nprint(\"=\"*70)\nprint(\"âœ“ Tokenization Complete\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:23:01.787984Z","iopub.execute_input":"2025-11-09T17:23:01.788560Z","iopub.status.idle":"2025-11-09T17:23:02.294072Z","shell.execute_reply.started":"2025-11-09T17:23:01.788538Z","shell.execute_reply":"2025-11-09T17:23:02.293389Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nTOKENIZING DATASETS\n======================================================================\n\nTokenizing train set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing train:   0%|          | 0/567 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac5027aca41b410981cee1ef0dc84117"}},"metadata":{}},{"name":"stdout","text":"Tokenizing validation set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing val:   0%|          | 0/122 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"330bb3031ff84b9d93a98f4a7222f932"}},"metadata":{}},{"name":"stdout","text":"Tokenizing test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing test:   0%|          | 0/122 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d148843302e46a698de656c0ab5d5e8"}},"metadata":{}},{"name":"stdout","text":"\n======================================================================\nTOKENIZATION STATISTICS\n======================================================================\nInput tokens - Min: 292, Max: 384, Avg: 307.5\nTarget tokens - Min: 25, Max: 96, Avg: 49.7\n\nTruncation: Inputs 28/567 (4.9%), Targets 25/567 (4.4%)\n======================================================================\nâœ“ Tokenization Complete\n======================================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================\n# CELL 7: TRAINING SETUP\n# ============================================================================\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\nimport time\n\nprint(\"=\"*70)\nprint(\"TRAINING SETUP\")\nprint(\"=\"*70)\n\n# Data collator for dynamic padding\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\n# Training arguments\n# Training arguments (OPTIMIZED)\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=str(OUTPUT_MODEL_DIR),\n    \n    # Batch sizes (OPTIMIZED)\n    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,  # 8 (was 4)\n    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,    # 8 (was 4)\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,   # 4 (was 8)\n    # Effective batch size = 8 * 4 = 32 (same)\n    \n    # Learning rate and optimizer (OPTIMIZED)\n    learning_rate=LEARNING_RATE,  # 2.5e-4 (was 1e-4) âš¡\n    optim=OPTIMIZER,\n    warmup_ratio=WARMUP_RATIO,    # 0.0 (was 0.1) âš¡\n    weight_decay=WEIGHT_DECAY,    # 0.0 (was 0.01) âš¡\n    \n    # Training duration\n    num_train_epochs=NUM_TRAIN_EPOCHS,\n    \n    # Evaluation and saving\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    save_total_limit=1,\n    \n    # Generation settings\n    predict_with_generate=True,\n    generation_max_length=MAX_TARGET_LENGTH,\n    generation_num_beams=GEN_NUM_BEAMS,  # 5\n    \n    # Precision and performance (OPTIMIZED)\n    fp16=False,  # Disabled for stability âš¡\n    \n    # Logging\n    logging_strategy=\"steps\",\n    logging_steps=LOGGING_STEPS,\n    report_to=\"none\",\n    \n    # Hardware optimization\n    dataloader_num_workers=0,\n    dataloader_pin_memory=False,\n)\n\n# Placeholder compute_metrics (detailed evaluation after training)\ndef compute_metrics(eval_pred):\n    return {}\n\n# Initialize trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING CONFIGURATION\")\nprint(\"=\"*70)\nprint(f\"Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Training samples: {len(tokenized_train)}\")\nprint(f\"Steps per epoch: {len(tokenized_train) // (PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)}\")\nprint(f\"Total training steps: ~{(len(tokenized_train) // (PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NUM_TRAIN_EPOCHS}\")\nprint(f\"Estimated time: ~{((len(tokenized_train) // (PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NUM_TRAIN_EPOCHS * 1.5 / 60):.0f} minutes\")\nprint(\"=\"*70)\nprint(\"âœ“ Trainer Ready\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:23:36.106044Z","iopub.execute_input":"2025-11-09T17:23:36.106385Z","iopub.status.idle":"2025-11-09T17:23:37.393453Z","shell.execute_reply.started":"2025-11-09T17:23:36.106361Z","shell.execute_reply":"2025-11-09T17:23:37.392781Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nTRAINING SETUP\n======================================================================\n\n======================================================================\nTRAINING CONFIGURATION\n======================================================================\nEffective batch size: 32\nTraining samples: 567\nSteps per epoch: 17\nTotal training steps: ~85\nEstimated time: ~2 minutes\n======================================================================\nâœ“ Trainer Ready\n======================================================================\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# CELL 8: TRAINING\n# ============================================================================\nprint(\"=\"*70)\nprint(\"STARTING FINE-TUNING\")\nprint(\"=\"*70)\nprint(f\"Model: FLAN-T5-Base + DAPT (Span-Masked) + CL\")\nprint(f\"Task: ABSA (7-field format)\")\nprint(f\"Dataset: {len(tokenized_train)} train, {len(tokenized_val)} val\")\nprint(\"=\"*70)\n\nstart_time = time.time()\nmodel.train()\n\n# Start training\ntrain_result = trainer.train()\n\nend_time = time.time()\ntraining_minutes = (end_time - start_time) / 60\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*70)\nprint(f\"Training time: {training_minutes:.1f} minutes ({training_minutes/60:.2f} hours)\")\nprint(f\"Final train loss: {train_result.metrics.get('train_loss', 'N/A'):.4f}\")\nprint(\"=\"*70)\n\n# Save metrics\ntrainer.log_metrics(\"train\", train_result.metrics)\ntrainer.save_metrics(\"train\", train_result.metrics)\n\n# Save best model\nprint(f\"\\nSaving best model to {OUTPUT_MODEL_DIR}...\")\ntrainer.save_model(str(OUTPUT_MODEL_DIR))\ntokenizer.save_pretrained(str(OUTPUT_MODEL_DIR))\nprint(\"âœ“ Model and tokenizer saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:23:49.906375Z","iopub.execute_input":"2025-11-09T17:23:49.906700Z","iopub.status.idle":"2025-11-09T17:31:34.327049Z","shell.execute_reply.started":"2025-11-09T17:23:49.906676Z","shell.execute_reply":"2025-11-09T17:31:34.326020Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nSTARTING FINE-TUNING\n======================================================================\nModel: FLAN-T5-Base + DAPT (Span-Masked) + CL\nTask: ABSA (7-field format)\nDataset: 567 train, 122 val\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [45/45 07:36, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.987630</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.726520</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.629680</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.595783</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.584395</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nTRAINING COMPLETE\n======================================================================\nTraining time: 7.7 minutes (0.13 hours)\nFinal train loss: 1.0871\n======================================================================\n***** train metrics *****\n  epoch                    =        5.0\n  total_flos               =  1303462GF\n  train_loss               =     1.0871\n  train_runtime            = 0:07:41.99\n  train_samples_per_second =      6.136\n  train_steps_per_second   =      0.097\n\nSaving best model to /kaggle/working/flan-t5-dapt-cl-synthetic-full-fft-optimized...\nâœ“ Model and tokenizer saved\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================\n# CELL 9: GENERATE PREDICTIONS ON TEST SET\n# ============================================================================\nimport torch\nfrom tqdm.notebook import tqdm\nimport json\n\nprint(\"=\"*70)\nprint(\"GENERATING PREDICTIONS\")\nprint(\"=\"*70)\n\n# Load best model\nprint(f\"Loading best model from {OUTPUT_MODEL_DIR}...\")\nmodel = T5ForConditionalGeneration.from_pretrained(OUTPUT_MODEL_DIR).to(device)\ntokenizer = T5TokenizerFast.from_pretrained(OUTPUT_MODEL_DIR)\nmodel.eval()\nprint(\"âœ“ Model loaded\\n\")\n\ndef generate_predictions(dataset, tokenizer, model, device, batch_size=PER_DEVICE_EVAL_BATCH_SIZE):\n    \"\"\"Generate predictions with progress bar\"\"\"\n    all_preds = []\n    \n    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Generating\"):\n        batch = dataset[i:i + batch_size]\n        input_texts = batch['input']\n        gold_targets = batch['target']\n        \n        # Tokenize\n        inputs = tokenizer(\n            input_texts, \n            return_tensors=\"pt\", \n            padding=True, \n            truncation=True, \n            max_length=MAX_INPUT_LENGTH\n        ).to(device)\n        \n        # Generate\n        with torch.no_grad():\n            generated_ids = model.generate(\n                **inputs,\n                num_beams=GEN_NUM_BEAMS,\n                max_length=MAX_TARGET_LENGTH,\n                early_stopping=True\n            )\n        \n        # Decode\n        generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n        \n        # Store results\n        for j in range(len(input_texts)):\n            all_preds.append({\n                'input': input_texts[j],\n                'gold_target': gold_targets[j],\n                'gen_output': generated_texts[j]\n            })\n    \n    return all_preds\n\n# Generate predictions\nprint(f\"Generating predictions on {len(test_ds)} test samples...\\n\")\ntest_predictions = generate_predictions(test_ds, tokenizer, model, device)\n\n# Save predictions\nPRED_FILE = OUTPUT_MODEL_DIR / \"test_predictions.jsonl\"\nwith open(PRED_FILE, 'w', encoding='utf-8') as f:\n    for pred in test_predictions:\n        f.write(json.dumps(pred, ensure_ascii=False) + '\\n')\n\nprint(f\"\\nâœ“ Predictions saved to {PRED_FILE}\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:33:14.865983Z","iopub.execute_input":"2025-11-09T17:33:14.866291Z","iopub.status.idle":"2025-11-09T17:33:53.240069Z","shell.execute_reply.started":"2025-11-09T17:33:14.866248Z","shell.execute_reply":"2025-11-09T17:33:53.239475Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nGENERATING PREDICTIONS\n======================================================================\nLoading best model from /kaggle/working/flan-t5-dapt-cl-synthetic-full-fft-optimized...\nâœ“ Model loaded\n\nGenerating predictions on 122 test samples...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5db5800a15264210a0e7ecc7d3065768"}},"metadata":{}},{"name":"stdout","text":"\nâœ“ Predictions saved to /kaggle/working/flan-t5-dapt-cl-synthetic-full-fft-optimized/test_predictions.jsonl\n======================================================================\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================================================\n# CELL 10: EVALUATE PREDICTIONS (7-FIELD FORMAT)\n# ============================================================================\nimport re\nfrom collections import Counter\n\nprint(\"=\"*70)\nprint(\"EVALUATION - 7-FIELD ABSA FORMAT\")\nprint(\"=\"*70)\n\n# ============================================================================\n# PARSING FUNCTIONS\n# ============================================================================\n\ndef normalize_aspect(s):\n    \"\"\"Normalize aspect term\"\"\"\n    if not s: return ''\n    return re.sub(r'\\s+', ' ', s.strip().lower())\n\ndef normalize_sentiment(s):\n    \"\"\"Normalize sentiment to positive/negative/neutral\"\"\"\n    if not s: return ''\n    s = s.strip().lower()\n    if s.startswith('pos'): return 'positive'\n    if s.startswith('neg'): return 'negative'\n    if s.startswith('neu'): return 'neutral'\n    return s\n\ndef parse_7field_output(text):\n    \"\"\"\n    Parse 7-field format: aspect | sentiment | span | rationale | is_implicit | has_slang | has_emoji\n    Returns: set of (aspect, sentiment) tuples for matching\n    \"\"\"\n    pairs = set()\n    if not text or not text.strip():\n        return pairs\n    \n    # Split by ' ; ' separator\n    entries = re.split(r'\\s*;\\s*', text.strip())\n    \n    for entry in entries:\n        # Split by ' | ' to get fields\n        fields = [f.strip() for f in entry.split('|')]\n        \n        if len(fields) >= 2:\n            aspect = normalize_aspect(fields[0])\n            sentiment = normalize_sentiment(fields[1])\n            \n            if aspect and sentiment in ['positive', 'negative', 'neutral']:\n                pairs.add((aspect, sentiment))\n    \n    return pairs\n\n# ============================================================================\n# METRICS COMPUTATION\n# ============================================================================\n\ndef compute_prf(tp, fp, fn):\n    \"\"\"Compute Precision, Recall, F1\"\"\"\n    p = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    r = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    f = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n    return p, r, f\n\n# Counters\nTP = FP = FN = 0  # Aspect + Sentiment (tuple matching)\nTP_aspect = FP_aspect = FN_aspect = 0  # Aspect-only\nexact_match = 0\ntotal_samples = len(test_predictions)\n\n# Sentiment accuracy on detected aspects\nsentiment_correct = 0\ndetected_aspects = 0\n\n# Per-aspect stats\naspect_counts_gold = Counter()\naspect_counts_pred = Counter()\naspect_tp = Counter()\n\nprint(f\"\\nEvaluating {total_samples} test samples...\\n\")\n\n# ============================================================================\n# EVALUATION LOOP\n# ============================================================================\n\nfor item in test_predictions:\n    gold_text = item['gold_target']\n    pred_text = item['gen_output']\n    \n    # Parse both\n    gold_pairs = parse_7field_output(gold_text)\n    pred_pairs = parse_7field_output(pred_text)\n    \n    # Tuple-level (aspect + sentiment)\n    TP += len(gold_pairs & pred_pairs)\n    FP += len(pred_pairs - gold_pairs)\n    FN += len(gold_pairs - pred_pairs)\n    \n    # Exact match\n    if gold_pairs == pred_pairs:\n        exact_match += 1\n    \n    # Aspect-only (ignore sentiment)\n    gold_aspects = set(a for a, _ in gold_pairs)\n    pred_aspects = set(a for a, _ in pred_pairs)\n    \n    TP_aspect += len(gold_aspects & pred_aspects)\n    FP_aspect += len(pred_aspects - gold_aspects)\n    FN_aspect += len(gold_aspects - pred_aspects)\n    \n    # Per-aspect counts\n    for asp in gold_aspects:\n        aspect_counts_gold[asp] += 1\n    for asp in pred_aspects:\n        aspect_counts_pred[asp] += 1\n    for asp in (gold_aspects & pred_aspects):\n        aspect_tp[asp] += 1\n    \n    # Sentiment accuracy on detected aspects\n    for asp in (gold_aspects & pred_aspects):\n        detected_aspects += 1\n        gold_sents = set(s for a, s in gold_pairs if a == asp)\n        pred_sents = set(s for a, s in pred_pairs if a == asp)\n        if gold_sents & pred_sents:\n            sentiment_correct += 1\n\n# ============================================================================\n# COMPUTE METRICS\n# ============================================================================\n\ntuple_p, tuple_r, tuple_f1 = compute_prf(TP, FP, FN)\naspect_p, aspect_r, aspect_f1 = compute_prf(TP_aspect, FP_aspect, FN_aspect)\nexact_acc = exact_match / total_samples if total_samples > 0 else 0.0\nsent_acc = sentiment_correct / detected_aspects if detected_aspects > 0 else 0.0\n\n# ============================================================================\n# DISPLAY RESULTS\n# ============================================================================\n\nprint(\"=\"*70)\nprint(\"PRIMARY METRICS\")\nprint(\"=\"*70)\nprint(f\"\\n1. Aspect + Sentiment (Tuple Matching):\")\nprint(f\"   TP: {TP}, FP: {FP}, FN: {FN}\")\nprint(f\"   Precision: {tuple_p:.4f}\")\nprint(f\"   Recall:    {tuple_r:.4f}\")\nprint(f\"   F1 Score:  {tuple_f1:.4f}\")\n\nprint(f\"\\n2. Aspect-Only (Ignore Sentiment):\")\nprint(f\"   TP: {TP_aspect}, FP: {FP_aspect}, FN: {FN_aspect}\")\nprint(f\"   Precision: {aspect_p:.4f}\")\nprint(f\"   Recall:    {aspect_r:.4f}\")\nprint(f\"   F1 Score:  {aspect_f1:.4f}\")\n\nprint(f\"\\n3. Exact Match Accuracy:\")\nprint(f\"   {exact_match}/{total_samples} = {exact_acc:.4f} ({exact_acc*100:.2f}%)\")\n\nprint(f\"\\n4. Sentiment Accuracy (on detected aspects):\")\nprint(f\"   {sentiment_correct}/{detected_aspects} = {sent_acc:.4f} ({sent_acc*100:.2f}%)\")\n\n# ============================================================================\n# TOP ASPECTS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TOP 10 ASPECTS\")\nprint(\"=\"*70)\n\ntop_aspects = aspect_counts_gold.most_common(10)\nprint(f\"\\n{'Aspect':<30} {'Gold':>6} {'Pred':>6} {'TP':>6} {'F1':>8}\")\nprint(\"-\"*70)\n\nfor asp, gold_count in top_aspects:\n    pred_count = aspect_counts_pred.get(asp, 0)\n    tp_count = aspect_tp.get(asp, 0)\n    \n    p = tp_count / pred_count if pred_count > 0 else 0.0\n    r = tp_count / gold_count if gold_count > 0 else 0.0\n    f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n    \n    print(f\"{asp:<30} {gold_count:>6} {pred_count:>6} {tp_count:>6} {f1:>8.4f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ“ EVALUATION COMPLETE\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:34:00.232527Z","iopub.execute_input":"2025-11-09T17:34:00.232803Z","iopub.status.idle":"2025-11-09T17:34:00.253434Z","shell.execute_reply.started":"2025-11-09T17:34:00.232783Z","shell.execute_reply":"2025-11-09T17:34:00.252672Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nEVALUATION - 7-FIELD ABSA FORMAT\n======================================================================\n\nEvaluating 122 test samples...\n\n======================================================================\nPRIMARY METRICS\n======================================================================\n\n1. Aspect + Sentiment (Tuple Matching):\n   TP: 54, FP: 71, FN: 82\n   Precision: 0.4320\n   Recall:    0.3971\n   F1 Score:  0.4138\n\n2. Aspect-Only (Ignore Sentiment):\n   TP: 70, FP: 55, FN: 65\n   Precision: 0.5600\n   Recall:    0.5185\n   F1 Score:  0.5385\n\n3. Exact Match Accuracy:\n   48/122 = 0.3934 (39.34%)\n\n4. Sentiment Accuracy (on detected aspects):\n   54/70 = 0.7714 (77.14%)\n\n======================================================================\nTOP 10 ASPECTS\n======================================================================\n\nAspect                           Gold   Pred     TP       F1\n----------------------------------------------------------------------\nsound_quality                       5      2      2   0.5714\ntaste                               5      3      3   0.7500\nbattery_life                        5      4      3   0.6667\natmosphere                          4      3      3   0.8571\nvibe                                4      2      2   0.6667\nfood_quality                        4      1      1   0.4000\nscreen_quality                      4      2      2   0.6667\nvariety                             4      2      2   0.6667\ndurability                          3      2      2   0.8000\nacting                              3      2      2   0.8000\n\n======================================================================\nâœ“ EVALUATION COMPLETE\n======================================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================================\n# CELL 11: SHOW PREDICTION EXAMPLES\n# ============================================================================\nimport re\n\ndef extract_actual_text(prompt):\n    \"\"\"Extract the actual text from prompt\"\"\"\n    matches = list(re.finditer(r'Text:\\s*(.+?)(?=\\s*Output:|$)', prompt, re.DOTALL))\n    if matches:\n        return matches[-1].group(1).strip()\n    return \"[Could not extract]\"\n\nprint(\"=\"*70)\nprint(\"PREDICTION EXAMPLES (First 10)\")\nprint(\"=\"*70)\n\nfor i in range(min(10, len(test_predictions))):\n    item = test_predictions[i]\n    actual_text = extract_actual_text(item['input'])\n    \n    print(f\"\\n{'â”€'*70}\")\n    print(f\"Example {i+1}\")\n    print(f\"{'â”€'*70}\")\n    print(f\"TEXT:\\n{actual_text[:200]}...\")\n    print(f\"\\nGOLD:\\n{item['gold_target']}\")\n    print(f\"\\nPREDICTED:\\n{item['gen_output']}\")\n    \n    # Quick match check\n    gold_pairs = parse_7field_output(item['gold_target'])\n    pred_pairs = parse_7field_output(item['gen_output'])\n    match = \"âœ… EXACT MATCH\" if gold_pairs == pred_pairs else f\"âŒ Partial ({len(gold_pairs & pred_pairs)}/{len(gold_pairs)} matched)\"\n    print(f\"\\n{match}\")\n\nprint(\"\\n\" + \"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:34:32.033933Z","iopub.execute_input":"2025-11-09T17:34:32.034201Z","iopub.status.idle":"2025-11-09T17:34:32.042172Z","shell.execute_reply.started":"2025-11-09T17:34:32.034181Z","shell.execute_reply":"2025-11-09T17:34:32.041389Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nPREDICTION EXAMPLES (First 10)\n======================================================================\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nExample 1\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTEXT:\ntried the new cafe downtown, food taste was meh but portions were huge tho, worth it for the deal...\n\nGOLD:\nfood_taste | neutral | [4, 4] | Explicit neutral: 'food taste was meh' indicates average flavor without strong praise or criticism. | FALSE | FALSE | FALSE\n\nPREDICTED:\nfood_taste | negative | [0, 1] | Explicit negative: 'food taste was meh' bland. | FALSE | FALSE | FALSE ; portions | negative | [0, 1] | Explicit negative: 'portions were huge' bland. | FALSE | FALSE | FALSE\n\nâŒ Partial (0/1 matched)\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nExample 2\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTEXT:\ntraffic on the highway is brutal rn, took me 90 mins for what should be 30 wtf ğŸš—...\n\nGOLD:\ntraffic | negative | [0, 1] | Explicit negative: 'traffic is brutal' with time delay shows irritation. | FALSE | TRUE | FALSE\n\nPREDICTED:\ntraffic | negative | [3, 3] | Explicit negative: 'traffic brutal' slow. | FALSE | FALSE | FALSE ; traffic | negative | [3, 3] | Explicit negative: 'traffic brutal' slow. | FALSE | FALSE | FALSE\n\nâœ… EXACT MATCH\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nExample 3\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTEXT:\noverall experience with the delivery app was smooth, tracked everything perfectly no issues...\n\nGOLD:\noverall_experience | positive | [0, 2] | Explicit positive: 'overall experience was smooth' summarizes positive interaction. | FALSE | FALSE | FALSE\n\nPREDICTED:\noverall_experience | positive | [0, 1] | Explicit positive: 'overall experience was smooth' reliable. | FALSE | FALSE | FALSE\n\nâœ… EXACT MATCH\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nExample 4\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTEXT:\nsound quality from these earbuds is decent for the price, bass hits okay not amazing...\n\nGOLD:\nsound_quality | neutral | [0, 2] | Explicit neutral: 'sound quality is decent' balances okay aspects without extremes. | FALSE | TRUE | FALSE\n\nPREDICTED:\nsound_quality | positive | [3, 3] | Explicit positive: 'sound quality is decent' for price. | FALSE | FALSE | FALSE ; sound_quality | positive | [3, 3] | Explicit positive: 'sound quality is decent' for price. | FALSE | FALSE | FALSE\n\nâŒ Partial (0/1 matched)\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nExample 5\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTEXT:\nservice speed at checkout was quick, in and out under 5 mins during rush hour...\n\nGOLD:\nservice_speed | positive | [0, 2] | Explicit positive: 'service speed was quick' notes efficiency. | FALSE | FALSE | FALSE\n\nPREDICTED:\nservice_speed | positive | [0, 1] | Explicit positive: 'service speed was quick' efficient. | FALSE | FALSE | FALSE\n\nâœ… EXACT MATCH\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nExample 6\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTEXT:\nspeed on the new router is blazing, downloads in seconds what took minutes before...\n\nGOLD:\nspeed | positive | [0, 1] | Explicit positive: 'speed is blazing' contrasts improvement. | FALSE | FALSE | FALSE\n\nPREDICTED:\nspeed | positive | [0, 1] | Explicit positive: 'speed blazing' efficient. | FALSE | FALSE | FALSE\n\nâœ… EXACT MATCH\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nExample 7\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTEXT:\natmosphere in the bar was chill, good spot to unwind after a long day ğŸ»...\n\nGOLD:\natmosphere | positive | [0, 1] | Explicit positive: 'atmosphere was chill' suits relaxation. | FALSE | FALSE | TRUE\n\nPREDICTED:\natmosphere | positive | [0, 1] | Explicit positive: 'atmosphere was chill' relaxing. | FALSE | FALSE | FALSE\n\nâœ… EXACT MATCH\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nExample 8\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTEXT:\ncleanliness at the hotel was spotless, rooms fresh and no dust anywhere...\n\nGOLD:\ncleanliness | positive | [0, 1] | Explicit positive: 'cleanliness was spotless' assures hygiene. | FALSE | TRUE | FALSE\n\nPREDICTED:\ncleanliness | positive | [0, 1] | Explicit positive: 'cleanliness spotless' clean. | FALSE | FALSE | FALSE\n\nâœ… EXACT MATCH\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nExample 9\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTEXT:\noh great, another update that breaks the app, thanks for nothing devs ğŸ™„...\n\nGOLD:\nsoftware | negative | [8, 8] | SARCASM: 'oh great' and 'thanks for nothing' mock the update causing issues. | FALSE | FALSE | TRUE\n\nPREDICTED:\napp_quality | positive | [0, 1] | Explicit positive: 'adding update breaks' useful. | FALSE | FALSE | FALSE ; app_quality | positive | [0, 1] | Explicit positive: 'adding update breaks' useful. | FALSE | FALSE | FALSE\n\nâŒ Partial (0/1 matched)\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nExample 10\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTEXT:\ndisplay on the tablet flickers sometimes, annoying during video calls...\n\nGOLD:\ndisplay | negative | [0, 1] | Explicit negative: 'display flickers' disrupts use. | FALSE | FALSE | FALSE\n\nPREDICTED:\ndisplay | negative | [0, 1] | Explicit negative: 'display flickers sometimes' annoying. | FALSE | FALSE | FALSE\n\nâœ… EXACT MATCH\n\n======================================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install bert_score\n# ============================================================================\n# CELL 12: GENERATION QUALITY METRICS\n# ============================================================================\nprint(\"=\"*70)\nprint(\"GENERATION QUALITY METRICS\")\nprint(\"=\"*70)\n\n# ============================================================================\n# 1. ROUGE SCORES\n# ============================================================================\nprint(\"\\n[1/4] Computing ROUGE scores...\")\nimport evaluate\n\ntry:\n    rouge = evaluate.load('rouge')\n    \n    gold_strings = [item['gold_target'] for item in test_predictions]\n    pred_strings = [item['gen_output'] for item in test_predictions]\n    \n    rouge_results = rouge.compute(\n        predictions=pred_strings,\n        references=gold_strings,\n        use_stemmer=True\n    )\n    \n    print(\"\\nâœ… ROUGE Scores:\")\n    print(f\"   ROUGE-1: {rouge_results['rouge1']:.4f}\")\n    print(f\"   ROUGE-2: {rouge_results['rouge2']:.4f}\")\n    print(f\"   ROUGE-L: {rouge_results['rougeL']:.4f}\")\n    \nexcept Exception as e:\n    print(f\"   âŒ ROUGE failed: {e}\")\n\n# ============================================================================\n# 2. BLEU SCORE\n# ============================================================================\nprint(\"\\n[2/4] Computing BLEU score...\")\ntry:\n    from sacrebleu import corpus_bleu\n    \n    # BLEU expects list of references (each ref is a list)\n    references = [[gold] for gold in gold_strings]\n    \n    bleu_result = corpus_bleu(pred_strings, references)\n    \n    print(f\"\\nâœ… BLEU Score: {bleu_result.score:.4f}\")\n    print(f\"   (BLEU-1: {bleu_result.precisions[0]:.2f}, \"\n          f\"BLEU-2: {bleu_result.precisions[1]:.2f}, \"\n          f\"BLEU-3: {bleu_result.precisions[2]:.2f}, \"\n          f\"BLEU-4: {bleu_result.precisions[3]:.2f})\")\n    \nexcept ImportError:\n    print(\"   âš ï¸ Installing sacrebleu...\")\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"-q\", \"sacrebleu\"], check=True)\n    from sacrebleu import corpus_bleu\n    references = [[gold] for gold in gold_strings]\n    bleu_result = corpus_bleu(pred_strings, references)\n    print(f\"\\nâœ… BLEU Score: {bleu_result.score:.4f}\")\nexcept Exception as e:\n    print(f\"   âŒ BLEU failed: {e}\")\n\n# ============================================================================\n# 3. METEOR SCORE\n# ============================================================================\nprint(\"\\n[3/4] Computing METEOR score...\")\ntry:\n    meteor = evaluate.load('meteor')\n    \n    meteor_result = meteor.compute(\n        predictions=pred_strings,\n        references=gold_strings\n    )\n    \n    print(f\"\\nâœ… METEOR Score: {meteor_result['meteor']:.4f}\")\n    \nexcept Exception as e:\n    print(f\"   âŒ METEOR failed: {e}\")\n\n# ============================================================================\n# 4. BERTScore\n# ============================================================================\nprint(\"\\n[4/4] Computing BERTScore (this may take a minute)...\")\ntry:\n    bertscore = evaluate.load('bertscore')\n    \n    bert_results = bertscore.compute(\n        predictions=pred_strings,\n        references=gold_strings,\n        lang='en',\n        model_type='microsoft/deberta-base-mnli',  # Faster than roberta-large\n        batch_size=16\n    )\n    \n    # Average scores\n    avg_precision = sum(bert_results['precision']) / len(bert_results['precision'])\n    avg_recall = sum(bert_results['recall']) / len(bert_results['recall'])\n    avg_f1 = sum(bert_results['f1']) / len(bert_results['f1'])\n    \n    print(f\"\\nâœ… BERTScore:\")\n    print(f\"   Precision: {avg_precision:.4f}\")\n    print(f\"   Recall:    {avg_recall:.4f}\")\n    print(f\"   F1:        {avg_f1:.4f}\")\n    \nexcept Exception as e:\n    print(f\"   âŒ BERTScore failed: {e}\")\n    print(\"   (This is optional - task-specific metrics are more important)\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ“ GENERATION METRICS COMPLETE\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:34:54.105026Z","iopub.execute_input":"2025-11-09T17:34:54.105741Z","iopub.status.idle":"2025-11-09T17:35:19.125386Z","shell.execute_reply.started":"2025-11-09T17:34:54.105710Z","shell.execute_reply":"2025-11-09T17:35:19.124575Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.53.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.5)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.36.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.0.9)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.10.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert_score) (1.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\n======================================================================\nGENERATION QUALITY METRICS\n======================================================================\n\n[1/4] Computing ROUGE scores...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0610e9621684e7c8943e6625c6ec849"}},"metadata":{}},{"name":"stdout","text":"\nâœ… ROUGE Scores:\n   ROUGE-1: 0.5583\n   ROUGE-2: 0.3612\n   ROUGE-L: 0.5459\n\n[2/4] Computing BLEU score...\n   âš ï¸ Installing sacrebleu...\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 51.8/51.8 kB 2.3 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 104.1/104.1 kB 4.3 MB/s eta 0:00:00\n\nâœ… BLEU Score: 83.1077\n\n[3/4] Computing METEOR score...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ab7b30aa9fa4bdababf3eabfbd021a5"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… METEOR Score: 0.6401\n\n[4/4] Computing BERTScore (this may take a minute)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2099788f33db43d5af824f68e6faaaca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00f713a56e804f8abde4c7540c07e5b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc88333093c463cb3776faffc071eb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acea32981c984ff1944c408547e52121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"611bb952b18a4d78a2339d8437e8e02f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/557M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff7b7243910642358ac3c25a7778c6ca"}},"metadata":{}},{"name":"stdout","text":"\nâœ… BERTScore:\n   Precision: 0.7776\n   Recall:    0.7614\n   F1:        0.7686\n\n======================================================================\nâœ“ GENERATION METRICS COMPLETE\n======================================================================\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================================================\n# CELL 13: SEMANTIC HALLUCINATION & COVERAGE ANALYSIS\n# ============================================================================\nimport re\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nprint(\"=\"*70)\nprint(\"SEMANTIC HALLUCINATION & COVERAGE ANALYSIS\")\nprint(\"=\"*70)\n\n# ============================================================================\n# LOAD SEMANTIC SIMILARITY MODEL\n# ============================================================================\nprint(\"\\n[0/4] Loading semantic similarity model...\")\ntry:\n    semantic_model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, lightweight\n    print(\"âœ“ Semantic model loaded\")\nexcept:\n    print(\"âš ï¸ Installing sentence-transformers...\")\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"-q\", \"sentence-transformers\"], check=True)\n    semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n    print(\"âœ“ Semantic model loaded\")\n\nSEMANTIC_THRESHOLD = 0.65  # Cosine similarity threshold (0.65-0.75 is good)\n\ndef is_semantically_similar(aspect1, aspect2, threshold=SEMANTIC_THRESHOLD):\n    \"\"\"Check if two aspects are semantically similar\"\"\"\n    # Handle exact match first\n    if aspect1.lower() == aspect2.lower():\n        return True, 1.0\n    \n    # Normalize underscores to spaces\n    asp1 = aspect1.replace('_', ' ')\n    asp2 = aspect2.replace('_', ' ')\n    \n    # Compute embeddings\n    emb1 = semantic_model.encode([asp1])\n    emb2 = semantic_model.encode([asp2])\n    \n    # Compute cosine similarity\n    sim = cosine_similarity(emb1, emb2)[0][0]\n    \n    return sim >= threshold, sim\n\ndef extract_actual_text(prompt):\n    \"\"\"Extract actual text from prompt\"\"\"\n    matches = list(re.finditer(r'Text:\\s*(.+?)(?=\\s*Output:|$)', prompt, re.DOTALL))\n    if matches:\n        return matches[-1].group(1).strip()\n    return \"\"\n\n# ============================================================================\n# 1. SEMANTIC HALLUCINATION DETECTION\n# ============================================================================\nprint(\"\\n[1/4] Analyzing hallucinations with semantic similarity...\")\n\nexplicit_correct = 0\nimplicit_semantically_valid = 0\ntrue_hallucinations = 0\ntotal_generated_aspects = 0\n\n# Store examples\nhallucination_examples = []\nsemantic_match_examples = []\n\nfor idx, item in enumerate(test_predictions):\n    actual_text = extract_actual_text(item['input']).lower()\n    gold_output = item['gold_target'].lower()\n    gen_output = item['gen_output'].lower()\n    \n    # Parse gold aspects\n    gold_aspects = set()\n    for entry in re.split(r'\\s*;\\s*', gold_output):\n        fields = entry.split('|')\n        if fields:\n            gold_aspects.add(fields[0].strip())\n    \n    # Check generated aspects\n    for entry in re.split(r'\\s*;\\s*', gen_output):\n        fields = entry.split('|')\n        if not fields or not fields[0].strip():\n            continue\n        \n        aspect = fields[0].strip()\n        total_generated_aspects += 1\n        \n        # Category 1: Explicit (appears in text)\n        if aspect.replace('_', ' ') in actual_text:\n            explicit_correct += 1\n        else:\n            # Category 2: Check semantic similarity with gold aspects\n            is_semantic_match = False\n            best_sim = 0.0\n            best_match = None\n            \n            for gold_asp in gold_aspects:\n                is_similar, sim = is_semantically_similar(aspect, gold_asp)\n                if is_similar and sim > best_sim:\n                    is_semantic_match = True\n                    best_sim = sim\n                    best_match = gold_asp\n            \n            if is_semantic_match:\n                implicit_semantically_valid += 1\n                if len(semantic_match_examples) < 5:\n                    semantic_match_examples.append({\n                        'generated': aspect,\n                        'gold': best_match,\n                        'similarity': best_sim,\n                        'text': actual_text[:100]\n                    })\n            else:\n                # Category 3: True hallucination (not in text, not similar to gold)\n                true_hallucinations += 1\n                if len(hallucination_examples) < 5:\n                    hallucination_examples.append({\n                        'generated': aspect,\n                        'gold_aspects': list(gold_aspects),\n                        'text': actual_text[:100]\n                    })\n\n# Compute rates\nsemantic_hallucination_rate = true_hallucinations / total_generated_aspects if total_generated_aspects > 0 else 0\nstrict_hallucination_rate = (true_hallucinations + implicit_semantically_valid) / total_generated_aspects if total_generated_aspects > 0 else 0\n\nprint(f\"\\nâœ… Semantic Hallucination Analysis:\")\nprint(f\"   Total generated aspects:           {total_generated_aspects}\")\nprint(f\"   Explicit (in text):                {explicit_correct} ({explicit_correct/total_generated_aspects*100:.1f}%)\")\nprint(f\"   Implicit but semantically valid:   {implicit_semantically_valid} ({implicit_semantically_valid/total_generated_aspects*100:.1f}%)\")\nprint(f\"   True hallucinations:               {true_hallucinations} ({semantic_hallucination_rate*100:.1f}%)\")\nprint(f\"\\n   ğŸ“Š Semantic Hallucination Rate:    {semantic_hallucination_rate:.4f} ({semantic_hallucination_rate*100:.1f}%)\")\nprint(f\"   ğŸ“Š Strict Hallucination Rate:      {strict_hallucination_rate:.4f} ({strict_hallucination_rate*100:.1f}%)\")\n\n# Show examples\nif semantic_match_examples:\n    print(f\"\\n   Examples of semantic matches (threshold={SEMANTIC_THRESHOLD}):\")\n    for i, ex in enumerate(semantic_match_examples, 1):\n        print(f\"   {i}. Generated: '{ex['generated']}' â‰ˆ Gold: '{ex['gold']}' (sim={ex['similarity']:.3f})\")\n\nif hallucination_examples:\n    print(f\"\\n   âš ï¸ Examples of true hallucinations:\")\n    for i, ex in enumerate(hallucination_examples, 1):\n        print(f\"   {i}. Generated: '{ex['generated']}' | Gold: {ex['gold_aspects']}\")\n        print(f\"      Text: {ex['text']}...\")\n\n# ============================================================================\n# 2. SEMANTIC COVERAGE ANALYSIS\n# ============================================================================\nprint(\"\\n[2/4] Analyzing coverage with semantic matching...\")\n\ntotal_gold_aspects = 0\nexact_matches = 0\nsemantic_matches = 0\nmissed_aspects = 0\n\ncoverage_examples = []\n\nfor item in test_predictions:\n    gold_pairs = parse_7field_output(item['gold_target'])\n    pred_pairs = parse_7field_output(item['gen_output'])\n    \n    gold_aspects = set(a for a, _ in gold_pairs)\n    pred_aspects = set(a for a, _ in pred_pairs)\n    \n    for gold_asp in gold_aspects:\n        total_gold_aspects += 1\n        \n        # Check for exact match\n        if gold_asp in pred_aspects:\n            exact_matches += 1\n        else:\n            # Check for semantic match\n            found_semantic = False\n            for pred_asp in pred_aspects:\n                is_similar, sim = is_semantically_similar(gold_asp, pred_asp)\n                if is_similar:\n                    semantic_matches += 1\n                    found_semantic = True\n                    break\n            \n            if not found_semantic:\n                missed_aspects += 1\n                if len(coverage_examples) < 5:\n                    coverage_examples.append({\n                        'missed': gold_asp,\n                        'predicted': list(pred_aspects),\n                        'text': extract_actual_text(item['input'])[:100]\n                    })\n\n# Compute coverage rates\nexact_coverage = exact_matches / total_gold_aspects if total_gold_aspects > 0 else 0\nsemantic_coverage = (exact_matches + semantic_matches) / total_gold_aspects if total_gold_aspects > 0 else 0\n\nprint(f\"\\nâœ… Semantic Coverage Analysis:\")\nprint(f\"   Total gold aspects:       {total_gold_aspects}\")\nprint(f\"   Exact matches:            {exact_matches} ({exact_coverage*100:.1f}%)\")\nprint(f\"   Semantic matches:         {semantic_matches} ({semantic_matches/total_gold_aspects*100:.1f}%)\")\nprint(f\"   Missed aspects:           {missed_aspects} ({missed_aspects/total_gold_aspects*100:.1f}%)\")\nprint(f\"\\n   ğŸ“Š Exact Coverage:        {exact_coverage:.4f} ({exact_coverage*100:.1f}%)\")\nprint(f\"   ğŸ“Š Semantic Coverage:     {semantic_coverage:.4f} ({semantic_coverage*100:.1f}%)\")\n\nif coverage_examples:\n    print(f\"\\n   âš ï¸ Examples of missed aspects:\")\n    for i, ex in enumerate(coverage_examples, 1):\n        print(f\"   {i}. Missed: '{ex['missed']}' | Predicted: {ex['predicted']}\")\n        print(f\"      Text: {ex['text']}...\")\n\n# ============================================================================\n# 3. FORMAT ADHERENCE (same as before)\n# ============================================================================\nprint(\"\\n[3/4] Checking format adherence...\")\n\nvalid_format = 0\nmalformed = 0\nempty_outputs = 0\n\nfor item in test_predictions:\n    gen_output = item['gen_output'].strip()\n    \n    if not gen_output:\n        empty_outputs += 1\n        continue\n    \n    entries = re.split(r'\\s*;\\s*', gen_output)\n    all_valid = True\n    \n    for entry in entries:\n        fields = entry.split('|')\n        if len(fields) != 7:\n            all_valid = False\n            break\n        \n        if len(fields) >= 2:\n            sent = fields[1].strip().lower()\n            if sent not in ['positive', 'negative', 'neutral']:\n                all_valid = False\n                break\n    \n    if all_valid:\n        valid_format += 1\n    else:\n        malformed += 1\n\nformat_adherence = valid_format / len(test_predictions) if test_predictions else 0\n\nprint(f\"\\nâœ… Format Adherence:\")\nprint(f\"   Valid 7-field format: {valid_format}/{len(test_predictions)} ({format_adherence*100:.1f}%)\")\nprint(f\"   Malformed outputs:    {malformed} ({malformed/len(test_predictions)*100:.1f}%)\")\nprint(f\"   Empty outputs:        {empty_outputs} ({empty_outputs/len(test_predictions)*100:.1f}%)\")\n\n# ============================================================================\n# 4. QUALITY SCORE\n# ============================================================================\nprint(\"\\n[4/4] Computing overall quality score...\")\n\n# Weighted quality score\nquality_score = (\n    0.4 * semantic_coverage +           # 40% weight on finding aspects\n    0.3 * (1 - semantic_hallucination_rate) +  # 30% weight on avoiding hallucinations\n    0.2 * format_adherence +            # 20% weight on format\n    0.1 * (explicit_correct / total_generated_aspects if total_generated_aspects > 0 else 0)  # 10% weight on groundedness\n)\n\nprint(f\"\\nâœ… Overall Quality Score: {quality_score:.4f} ({quality_score*100:.1f}%)\")\nprint(f\"   Interpretation:\")\nif quality_score >= 0.80:\n    print(f\"   ğŸŸ¢ Excellent (â‰¥80%) - Production ready\")\nelif quality_score >= 0.65:\n    print(f\"   ğŸŸ¡ Good (65-80%) - Acceptable for research\")\nelif quality_score >= 0.50:\n    print(f\"   ğŸŸ  Fair (50-65%) - Needs improvement\")\nelse:\n    print(f\"   ğŸ”´ Poor (<50%) - Requires significant work\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ“ SEMANTIC ANALYSIS COMPLETE\")\nprint(\"=\"*70)\n\n# Store semantic metrics\nsemantic_metrics = {\n    'semantic_hallucination_rate': semantic_hallucination_rate,\n    'strict_hallucination_rate': strict_hallucination_rate,\n    'semantic_coverage': semantic_coverage,\n    'exact_coverage': exact_coverage,\n    'quality_score': quality_score\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:35:19.127080Z","iopub.execute_input":"2025-11-09T17:35:19.127336Z","iopub.status.idle":"2025-11-09T17:35:22.974054Z","shell.execute_reply.started":"2025-11-09T17:35:19.127318Z","shell.execute_reply":"2025-11-09T17:35:22.973435Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nSEMANTIC HALLUCINATION & COVERAGE ANALYSIS\n======================================================================\n\n[0/4] Loading semantic similarity model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dddf8c5262c4a9e93885fd6ee1982be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14302a0538424bb88de6192ba283ae6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8779429794664bd88efcb9f1b24850d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e23a6e5c7bb14f73a3dcbe44fa5d109b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef1d9cf83f447058b5bba7c20012360"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b20f4dc2e01435b8be534eacc4e1d7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fb65149a1654e1ea45eaea2bcbc8287"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c80ea5b1d2ff48078f02f1f2054489de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a546b6729b64911bf5f9b1e846b3293"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c587b3424fea4b5b9902d44166573ff8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfae484ba6a84bd5b76c06b634fd6518"}},"metadata":{}},{"name":"stdout","text":"âœ“ Semantic model loaded\n\n[1/4] Analyzing hallucinations with semantic similarity...\n\nâœ… Semantic Hallucination Analysis:\n   Total generated aspects:           140\n   Explicit (in text):                108 (77.1%)\n   Implicit but semantically valid:   8 (5.7%)\n   True hallucinations:               24 (17.1%)\n\n   ğŸ“Š Semantic Hallucination Rate:    0.1714 (17.1%)\n   ğŸ“Š Strict Hallucination Rate:      0.2286 (22.9%)\n\n   Examples of semantic matches (threshold=0.65):\n   1. Generated: 'policy_policy' â‰ˆ Gold: 'healthcare_policy' (sim=0.708)\n   2. Generated: 'support_reply' â‰ˆ Gold: 'support' (sim=0.753)\n   3. Generated: 'battery_life' â‰ˆ Gold: 'battery_life' (sim=1.000)\n   4. Generated: 'battery_life' â‰ˆ Gold: 'battery_life' (sim=1.000)\n   5. Generated: 'food_taste' â‰ˆ Gold: 'food_taste' (sim=1.000)\n\n   âš ï¸ Examples of true hallucinations:\n   1. Generated: 'app_quality' | Gold: ['software']\n      Text: oh great, another update that breaks the app, thanks for nothing devs ğŸ™„...\n   2. Generated: 'app_quality' | Gold: ['software']\n      Text: oh great, another update that breaks the app, thanks for nothing devs ğŸ™„...\n   3. Generated: 'benefits' | Gold: ['government_spending']\n      Text: awesome how the new law helps small businesses, more grants available now...\n   4. Generated: 'membership_quality' | Gold: ['value']\n      Text: gym membership renewal came with free classes, made the price jump tolerable...\n   5. Generated: 'plot' | Gold: ['pacing']\n      Text: episode pacing rushed the climax, felt crammed in last 10 mins...\n\n[2/4] Analyzing coverage with semantic matching...\n\nâœ… Semantic Coverage Analysis:\n   Total gold aspects:       135\n   Exact matches:            70 (51.9%)\n   Semantic matches:         12 (8.9%)\n   Missed aspects:           53 (39.3%)\n\n   ğŸ“Š Exact Coverage:        0.5185 (51.9%)\n   ğŸ“Š Semantic Coverage:     0.6074 (60.7%)\n\n   âš ï¸ Examples of missed aspects:\n   1. Missed: 'software' | Predicted: ['app_quality']\n      Text: oh great, another update that breaks the app, thanks for nothing devs ğŸ™„...\n   2. Missed: 'government_spending' | Predicted: ['benefits']\n      Text: awesome how the new law helps small businesses, more grants available now...\n   3. Missed: 'value' | Predicted: ['membership_quality']\n      Text: gym membership renewal came with free classes, made the price jump tolerable...\n   4. Missed: 'pacing' | Predicted: ['plot']\n      Text: episode pacing rushed the climax, felt crammed in last 10 mins...\n   5. Missed: 'professionalism' | Predicted: ['service']\n      Text: professional service at salon, cuts precise no second guess...\n\n[3/4] Checking format adherence...\n\nâœ… Format Adherence:\n   Valid 7-field format: 121/122 (99.2%)\n   Malformed outputs:    1 (0.8%)\n   Empty outputs:        0 (0.0%)\n\n[4/4] Computing overall quality score...\n\nâœ… Overall Quality Score: 0.7670 (76.7%)\n   Interpretation:\n   ğŸŸ¡ Good (65-80%) - Acceptable for research\n\n======================================================================\nâœ“ SEMANTIC ANALYSIS COMPLETE\n======================================================================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================================\n# CELL 14: FINAL RESULTS SUMMARY (WITH SEMANTIC METRICS)\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPREHENSIVE EVALUATION SUMMARY\")\nprint(\"=\"*70)\nprint(f\"\\nModel: FLAN-T5-Base + DAPT + CL\")\nprint(f\"Dataset: {len(test_predictions)} test samples\")\nprint(f\"Training time: {training_minutes:.1f} minutes\")\nprint(f\"Final train loss: {train_result.metrics.get('train_loss', 0):.4f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TASK-SPECIFIC METRICS (Primary)\")\nprint(\"=\"*70)\nprint(f\"\\n1. Aspect + Sentiment F1:      {tuple_f1:.4f}\")\nprint(f\"2. Aspect-Only F1:              {aspect_f1:.4f}\")\nprint(f\"3. Exact Match Accuracy:        {exact_acc:.4f}\")\nprint(f\"4. Sentiment Accuracy:          {sent_acc:.4f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"GENERATION QUALITY METRICS\")\nprint(\"=\"*70)\ntry:\n    print(f\"\\n5. ROUGE-L:                     {rouge_results['rougeL']:.4f}\")\n    print(f\"6. BLEU:                        {bleu_result.score:.4f}\")\n    print(f\"7. METEOR:                      {meteor_result['meteor']:.4f}\")\n    print(f\"8. BERTScore F1:                {avg_f1:.4f}\")\nexcept:\n    print(\"\\n(Some generation metrics unavailable)\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"RELIABILITY METRICS (Semantic-Aware)\")\nprint(\"=\"*70)\nprint(f\"\\n9. Semantic Hallucination Rate: {semantic_hallucination_rate:.4f} ({semantic_hallucination_rate*100:.1f}%)\")\nprint(f\"   (Strict Hallucination Rate:  {strict_hallucination_rate:.4f} ({strict_hallucination_rate*100:.1f}%))\")\nprint(f\"\\n10. Semantic Coverage:          {semantic_coverage:.4f} ({semantic_coverage*100:.1f}%)\")\nprint(f\"    (Exact Coverage:             {exact_coverage:.4f} ({exact_coverage*100:.1f}%))\")\nprint(f\"\\n11. Format Adherence:           {format_adherence:.4f} ({format_adherence*100:.1f}%)\")\nprint(f\"\\n12. Overall Quality Score:      {quality_score:.4f} ({quality_score*100:.1f}%)\")\n\n# Quality interpretation\nprint(f\"\\n    Quality Assessment: \", end=\"\")\nif quality_score >= 0.80:\n    print(f\"ğŸŸ¢ Excellent (Production Ready)\")\nelif quality_score >= 0.65:\n    print(f\"ğŸŸ¡ Good (Research Acceptable)\")\nelif quality_score >= 0.50:\n    print(f\"ğŸŸ  Fair (Needs Improvement)\")\nelse:\n    print(f\"ğŸ”´ Poor (Requires Major Work)\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ASPECT DETECTION BREAKDOWN\")\nprint(\"=\"*70)\nprint(f\"\\nTotal generated aspects: {total_generated_aspects}\")\nprint(f\"  â”œâ”€ Explicit (in text):              {explicit_correct} ({explicit_correct/total_generated_aspects*100:.1f}%)\")\nprint(f\"  â”œâ”€ Implicit (semantic match):       {implicit_semantically_valid} ({implicit_semantically_valid/total_generated_aspects*100:.1f}%)\")\nprint(f\"  â””â”€ True hallucinations:             {true_hallucinations} ({semantic_hallucination_rate*100:.1f}%)\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… EVALUATION COMPLETE\")\nprint(\"=\"*70)\n\n# ============================================================================\n# SAVE COMPREHENSIVE SUMMARY\n# ============================================================================\nsummary = {\n    'metadata': {\n        'model': 'FLAN-T5-Base + DAPT (Span-Masked)',\n        'dataset_size': len(test_predictions),\n        'training_time_minutes': float(training_minutes),\n        'train_loss': float(train_result.metrics.get('train_loss', 0)),\n        'evaluation_date': '2025-11-09',\n        'user': 'saheenus-pg'\n    },\n    \n    'task_specific_metrics': {\n        'aspect_sentiment_f1': float(tuple_f1),\n        'aspect_only_f1': float(aspect_f1),\n        'exact_match_accuracy': float(exact_acc),\n        'sentiment_accuracy': float(sent_acc)\n    },\n    \n    'generation_quality_metrics': {},\n    \n    'reliability_metrics': {\n        'semantic_hallucination_rate': float(semantic_hallucination_rate),\n        'strict_hallucination_rate': float(strict_hallucination_rate),\n        'semantic_coverage': float(semantic_coverage),\n        'exact_coverage': float(exact_coverage),\n        'format_adherence': float(format_adherence),\n        'overall_quality_score': float(quality_score)\n    },\n    \n    'aspect_breakdown': {\n        'total_generated': int(total_generated_aspects),\n        'explicit_in_text': int(explicit_correct),\n        'implicit_semantic_match': int(implicit_semantically_valid),\n        'true_hallucinations': int(true_hallucinations)\n    },\n    \n    'interpretation': {\n        'quality_level': 'Excellent' if quality_score >= 0.80 else \n                        'Good' if quality_score >= 0.65 else \n                        'Fair' if quality_score >= 0.50 else 'Poor',\n        'production_ready': bool(quality_score >= 0.80),\n        'research_acceptable': bool(quality_score >= 0.65)\n    }\n}\n\n# Add generation metrics if available\ntry:\n    summary['generation_quality_metrics'] = {\n        'rouge_l': float(rouge_results['rougeL']),\n        'bleu': float(bleu_result.score),\n        'meteor': float(meteor_result['meteor']),\n        'bertscore_f1': float(avg_f1)\n    }\nexcept:\n    summary['generation_quality_metrics'] = {\n        'note': 'Some metrics unavailable'\n    }\n\n# Save to JSON\nsummary_file = OUTPUT_MODEL_DIR / 'eval_summary_semantic.json'\nwith open(summary_file, 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint(f\"\\nâœ“ Comprehensive summary saved to {summary_file}\")\n\n# ============================================================================\n# SAVE HUMAN-READABLE REPORT\n# ============================================================================\nreport_file = OUTPUT_MODEL_DIR / 'evaluation_report.txt'\nwith open(report_file, 'w') as f:\n    f.write(\"=\"*70 + \"\\n\")\n    f.write(\"ABSA MODEL EVALUATION REPORT\\n\")\n    f.write(\"=\"*70 + \"\\n\")\n    f.write(f\"\\nModel: FLAN-T5-Base + DAPT (Span-Masked)\\n\")\n    f.write(f\"Evaluated: 2025-11-09\\n\")\n    f.write(f\"By: saheenus-pg\\n\")\n    f.write(f\"Test Samples: {len(test_predictions)}\\n\")\n    f.write(f\"Training Time: {training_minutes:.1f} minutes\\n\")\n    f.write(f\"Final Train Loss: {train_result.metrics.get('train_loss', 0):.4f}\\n\")\n    \n    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n    f.write(\"KEY FINDINGS\\n\")\n    f.write(\"=\"*70 + \"\\n\")\n    f.write(f\"\\nâœ“ Overall Quality: {quality_score:.1%} ({summary['interpretation']['quality_level']})\\n\")\n    f.write(f\"âœ“ Aspect Detection F1: {aspect_f1:.1%}\\n\")\n    f.write(f\"âœ“ Semantic Coverage: {semantic_coverage:.1%}\\n\")\n    f.write(f\"âœ“ Hallucination Rate: {semantic_hallucination_rate:.1%}\\n\")\n    f.write(f\"âœ“ Format Adherence: {format_adherence:.1%}\\n\")\n    \n    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n    f.write(\"DETAILED METRICS\\n\")\n    f.write(\"=\"*70 + \"\\n\")\n    f.write(f\"\\nTask-Specific:\\n\")\n    f.write(f\"  - Aspect + Sentiment F1: {tuple_f1:.4f}\\n\")\n    f.write(f\"  - Aspect-Only F1: {aspect_f1:.4f}\\n\")\n    f.write(f\"  - Exact Match Accuracy: {exact_acc:.4f}\\n\")\n    f.write(f\"  - Sentiment Accuracy: {sent_acc:.4f}\\n\")\n    \n    f.write(f\"\\nReliability:\\n\")\n    f.write(f\"  - Semantic Hallucination Rate: {semantic_hallucination_rate:.4f}\\n\")\n    f.write(f\"  - Semantic Coverage: {semantic_coverage:.4f}\\n\")\n    f.write(f\"  - Format Adherence: {format_adherence:.4f}\\n\")\n    \n    f.write(f\"\\nAspect Breakdown:\\n\")\n    f.write(f\"  - Explicit (in text): {explicit_correct}/{total_generated_aspects} ({explicit_correct/total_generated_aspects*100:.1f}%)\\n\")\n    f.write(f\"  - Implicit (semantic): {implicit_semantically_valid}/{total_generated_aspects} ({implicit_semantically_valid/total_generated_aspects*100:.1f}%)\\n\")\n    f.write(f\"  - Hallucinations: {true_hallucinations}/{total_generated_aspects} ({semantic_hallucination_rate*100:.1f}%)\\n\")\n    \n    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n    f.write(\"RECOMMENDATIONS\\n\")\n    f.write(\"=\"*70 + \"\\n\")\n    \n    if quality_score >= 0.80:\n        f.write(\"\\nâœ… Model is production-ready for ABSA tasks.\\n\")\n    elif quality_score >= 0.65:\n        f.write(\"\\nâœ… Model is acceptable for research purposes.\\n\")\n        f.write(\"ğŸ’¡ Suggestions for improvement:\\n\")\n        if semantic_coverage < 0.75:\n            f.write(\"   - Increase training data to improve coverage\\n\")\n        if semantic_hallucination_rate > 0.10:\n            f.write(\"   - Train longer or add regularization to reduce hallucinations\\n\")\n    else:\n        f.write(\"\\nâš ï¸ Model needs significant improvement.\\n\")\n        f.write(\"ğŸ’¡ Critical actions:\\n\")\n        f.write(\"   - Collect more diverse training data\\n\")\n        f.write(\"   - Train for more epochs (10-15)\\n\")\n        f.write(\"   - Consider simplifying output format\\n\")\n    \n    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n\nprint(f\"âœ“ Human-readable report saved to {report_file}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ“Š ALL EVALUATION ARTIFACTS SAVED\")\nprint(\"=\"*70)\nprint(f\"  1. {summary_file.name}\")\nprint(f\"  2. {report_file.name}\")\nprint(f\"  3. test_predictions.jsonl\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T17:36:23.832368Z","iopub.execute_input":"2025-11-09T17:36:23.833156Z","iopub.status.idle":"2025-11-09T17:36:23.855495Z","shell.execute_reply.started":"2025-11-09T17:36:23.833130Z","shell.execute_reply":"2025-11-09T17:36:23.854842Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nCOMPREHENSIVE EVALUATION SUMMARY\n======================================================================\n\nModel: FLAN-T5-Base + DAPT + CL\nDataset: 122 test samples\nTraining time: 7.7 minutes\nFinal train loss: 1.0871\n\n======================================================================\nTASK-SPECIFIC METRICS (Primary)\n======================================================================\n\n1. Aspect + Sentiment F1:      0.4138\n2. Aspect-Only F1:              0.5385\n3. Exact Match Accuracy:        0.3934\n4. Sentiment Accuracy:          0.7714\n\n======================================================================\nGENERATION QUALITY METRICS\n======================================================================\n\n5. ROUGE-L:                     0.5459\n6. BLEU:                        83.1077\n7. METEOR:                      0.6401\n8. BERTScore F1:                0.7686\n\n======================================================================\nRELIABILITY METRICS (Semantic-Aware)\n======================================================================\n\n9. Semantic Hallucination Rate: 0.1714 (17.1%)\n   (Strict Hallucination Rate:  0.2286 (22.9%))\n\n10. Semantic Coverage:          0.6074 (60.7%)\n    (Exact Coverage:             0.5185 (51.9%))\n\n11. Format Adherence:           0.9918 (99.2%)\n\n12. Overall Quality Score:      0.7670 (76.7%)\n\n    Quality Assessment: ğŸŸ¡ Good (Research Acceptable)\n\n======================================================================\nASPECT DETECTION BREAKDOWN\n======================================================================\n\nTotal generated aspects: 140\n  â”œâ”€ Explicit (in text):              108 (77.1%)\n  â”œâ”€ Implicit (semantic match):       8 (5.7%)\n  â””â”€ True hallucinations:             24 (17.1%)\n\n======================================================================\nâœ… EVALUATION COMPLETE\n======================================================================\n\nâœ“ Comprehensive summary saved to /kaggle/working/flan-t5-dapt-cl-synthetic-full-fft-optimized/eval_summary_semantic.json\nâœ“ Human-readable report saved to /kaggle/working/flan-t5-dapt-cl-synthetic-full-fft-optimized/evaluation_report.txt\n\n======================================================================\nğŸ“Š ALL EVALUATION ARTIFACTS SAVED\n======================================================================\n  1. eval_summary_semantic.json\n  2. evaluation_report.txt\n  3. test_predictions.jsonl\n======================================================================\n","output_type":"stream"}],"execution_count":14}]}